{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, Concatenate, Layer\n",
    ")\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = 'Nirmala UI' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_location = '/kaggle/input/as3-dataset/lexicons'):\n",
    "    # Helper function to load a TSV file and return as numpy array\n",
    "    def load_tsv(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                data.append(line.strip().split('\\t'))\n",
    "        return numpy.array(data, dtype=object)\n",
    "\n",
    "    data = {}\n",
    "    # Load train, dev, and test splits from the specified directory\n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        file_name = f\"gu.translit.sampled.{split}.tsv\"\n",
    "        file_path = os.path.join(data_location, file_name)\n",
    "        data[split] = load_tsv(file_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def tokanize_texts(texts, char_level=True, start_end_tokens=False):\n",
    "    # Optionally add start/end tokens to each text\n",
    "    START_TOKEN = '§'\n",
    "    END_TOKEN = '¶'\n",
    "    if start_end_tokens:\n",
    "        texts = [START_TOKEN + text + END_TOKEN for text in texts]\n",
    "    # Create a Keras tokenizer at character level (no filtering, case-sensitive)\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=char_level, filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "    \n",
    "def add_start_end(texts):\n",
    "    # Add start and end tokens to each text in the list\n",
    "    START_TOKEN = '§'\n",
    "    END_TOKEN = '¶'\n",
    "    return [START_TOKEN + text + END_TOKEN for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class seq2seq:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        output_vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_units,\n",
    "        encoder_layers,\n",
    "        decoder_layers,\n",
    "        dropout_rate,\n",
    "        recurrent_dropout_rate,\n",
    "        encoder_type,\n",
    "        decoder_type,\n",
    "        beam_width\n",
    "    ):\n",
    "        # Store model hyperparameters for later use\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.recurrent_dropout_rate = recurrent_dropout_rate\n",
    "        self.beam_width = beam_width\n",
    "        self.encoder_type = encoder_type\n",
    "        self.decoder_type = decoder_type\n",
    "    \n",
    "    def build_training_model(self):\n",
    "        # Encoder input and embedding layer\n",
    "        encoder_inputs = keras.layers.Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=self.input_vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            name='encoder_embedding'\n",
    "        )(encoder_inputs)\n",
    "\n",
    "        encoder_states = []\n",
    "        encoder_outputs = encoder_embedding\n",
    "\n",
    "        # Stack encoder RNN layers (LSTM/GRU/RNN)\n",
    "        for i in range(self.encoder_layers):\n",
    "            return_sequences = (i < self.encoder_layers - 1)  # Only last layer returns last state\n",
    "            return_state = True\n",
    "\n",
    "            if self.encoder_type == 'LSTM':\n",
    "                rnn_layer = keras.layers.LSTM(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'encoder_{i}'\n",
    "                )\n",
    "                encoder_outputs, state_h, state_c = rnn_layer(encoder_outputs)\n",
    "                encoder_states.extend([state_h, state_c])\n",
    "            elif self.encoder_type == 'GRU':\n",
    "                rnn_layer = keras.layers.GRU(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'encoder_{i}'\n",
    "                )\n",
    "                encoder_outputs, state_h = rnn_layer(encoder_outputs)\n",
    "                encoder_states.append(state_h)\n",
    "            elif self.encoder_type == 'SimpleRNN':\n",
    "                rnn_layer = keras.layers.SimpleRNN(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'encoder_{i}'\n",
    "                )\n",
    "                encoder_outputs, state_h = rnn_layer(encoder_outputs)\n",
    "                encoder_states.append(state_h)\n",
    "        \n",
    "        # Decoder input and embedding layer\n",
    "        decoder_inputs = keras.layers.Input(shape=(None,), name='decoder_inputs')\n",
    "        decoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=self.output_vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            name='decoder_embedding'\n",
    "        )(decoder_inputs)\n",
    "\n",
    "        decoder_outputs = decoder_embedding\n",
    "        decoder_init_states = []\n",
    "\n",
    "        # Prepare initial decoder states from encoder final states\n",
    "        idx = 0\n",
    "        for i in range(self.decoder_layers):\n",
    "            if i < self.encoder_layers:\n",
    "                if self.decoder_type == 'LSTM':\n",
    "                    h = encoder_states[idx]\n",
    "                    c = encoder_states[idx + 1]\n",
    "                    decoder_init_states.append([h, c])\n",
    "                    idx += 2\n",
    "                else:\n",
    "                    h = encoder_states[idx]\n",
    "                    decoder_init_states.append([h])\n",
    "                    idx += 1\n",
    "            else:\n",
    "                # If decoder has more layers than encoder, repeat last encoder state\n",
    "                if self.decoder_type == 'LSTM':\n",
    "                    h = encoder_states[-2]\n",
    "                    c = encoder_states[-1]\n",
    "                    decoder_init_states.append([h, c])\n",
    "                else:\n",
    "                    h = encoder_states[-1]\n",
    "                    decoder_init_states.append([h])\n",
    "\n",
    "        # Stack decoder RNN layers (LSTM/GRU/RNN)\n",
    "        for i in range(self.decoder_layers):\n",
    "            return_sequences = True\n",
    "            return_state = True\n",
    "\n",
    "            if self.decoder_type == 'LSTM':\n",
    "                rnn_layer = keras.layers.LSTM(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'decoder_{i}'\n",
    "                )\n",
    "                decoder_outputs, _, _ = rnn_layer(decoder_outputs, initial_state=decoder_init_states[i])\n",
    "            elif self.decoder_type == 'GRU':\n",
    "                rnn_layer = keras.layers.GRU(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'decoder_{i}'\n",
    "                )\n",
    "                decoder_outputs, _ = rnn_layer(decoder_outputs, initial_state=decoder_init_states[i])\n",
    "            elif self.decoder_type == 'SimpleRNN':\n",
    "                rnn_layer = keras.layers.SimpleRNN(\n",
    "                    units=self.hidden_units,\n",
    "                    return_sequences=return_sequences,\n",
    "                    return_state=return_state,\n",
    "                    dropout=self.dropout_rate,\n",
    "                    recurrent_dropout=self.recurrent_dropout_rate,\n",
    "                    name=f'decoder_{i}'\n",
    "                )\n",
    "                decoder_outputs, _ = rnn_layer(decoder_outputs, initial_state=decoder_init_states[i])\n",
    "\n",
    "        # Output layer: projects decoder outputs to vocabulary size\n",
    "        decoder_dense = keras.layers.Dense(\n",
    "            units=self.output_vocab_size,\n",
    "            activation='softmax',\n",
    "            name='decoder_dense'\n",
    "        )\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        # Define the full training model (encoder + decoder)\n",
    "        self.training_model = keras.models.Model(\n",
    "            inputs=[encoder_inputs, decoder_inputs],\n",
    "            outputs=decoder_outputs\n",
    "        )\n",
    "    \n",
    "    def build_inference_model(self):\n",
    "        # Build encoder inference model for prediction (single step at a time)\n",
    "        encoder_inputs = keras.layers.Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding_layer = self.training_model.get_layer('encoder_embedding')\n",
    "        encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "        encoder_outputs = encoder_embedding\n",
    "        encoder_states = []\n",
    "        # Run through encoder layers to get final states for inference\n",
    "        for i in range(min(self.encoder_layers, self.decoder_layers)):\n",
    "            encoder_rnn_layer = self.training_model.get_layer(f'encoder_{i}')\n",
    "            encoder_outputs, *state = encoder_rnn_layer(encoder_outputs)\n",
    "            encoder_states.extend(state)\n",
    "\n",
    "        # Encoder model outputs all encoder states needed for decoder initialization\n",
    "        self.encoder_model = keras.models.Model(\n",
    "            inputs=encoder_inputs,\n",
    "            outputs=encoder_states\n",
    "        )\n",
    "\n",
    "        # Build decoder inference model for step-by-step prediction\n",
    "        decoder_inputs = keras.layers.Input(shape=(None,), name='decoder_inputs')\n",
    "        decoder_embedding_layer = self.training_model.get_layer('decoder_embedding')\n",
    "        decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "        decoder_states_inputs = []\n",
    "        # Prepare input placeholders for decoder's initial states at each layer\n",
    "        for idx, state in enumerate(encoder_states):\n",
    "            decoder_states_inputs.append(\n",
    "                keras.layers.Input(shape=(self.hidden_units,), name=f'decoder_state_input_{idx}')\n",
    "            )\n",
    "\n",
    "        decoder_outputs = decoder_embedding\n",
    "        decoder_states = []\n",
    "\n",
    "        state_idx = 0\n",
    "        # Rebuild decoder RNN stack for inference, using state inputs\n",
    "        for i in range(self.decoder_layers):\n",
    "            decoder_rnn_layer = self.training_model.get_layer(f'decoder_{i}')\n",
    "            if i < self.encoder_layers:\n",
    "                if self.decoder_type == 'LSTM':\n",
    "                    init_h = decoder_states_inputs[state_idx]\n",
    "                    init_c = decoder_states_inputs[state_idx + 1]\n",
    "                    decoder_outputs, state_h, state_c = decoder_rnn_layer(\n",
    "                        decoder_outputs, initial_state=[init_h, init_c]\n",
    "                    )\n",
    "                    decoder_states.extend([state_h, state_c])\n",
    "                    state_idx += 2\n",
    "                else:\n",
    "                    init_h = decoder_states_inputs[state_idx]\n",
    "                    decoder_outputs, state_h = decoder_rnn_layer(\n",
    "                        decoder_outputs, initial_state=[init_h]\n",
    "                    )\n",
    "                    decoder_states.append(state_h)\n",
    "                    state_idx += 1\n",
    "            else:\n",
    "                # For extra decoder layers, repeat last encoder state\n",
    "                if self.decoder_type == 'LSTM':\n",
    "                    init_h = decoder_states_inputs[-2]\n",
    "                    init_c = decoder_states_inputs[-1]\n",
    "                    decoder_outputs, state_h, state_c = decoder_rnn_layer(\n",
    "                        decoder_outputs, initial_state=[init_h, init_c]\n",
    "                    )\n",
    "                    decoder_states.extend([state_h, state_c])\n",
    "                else:\n",
    "                    init_h = decoder_states_inputs[-1]\n",
    "                    decoder_outputs, state_h = decoder_rnn_layer(\n",
    "                        decoder_outputs, initial_state=[init_h]\n",
    "                    )\n",
    "                    decoder_states.append(state_h)\n",
    "\n",
    "        decoder_dense_layer = self.training_model.get_layer('decoder_dense')\n",
    "        decoder_outputs = decoder_dense_layer(decoder_outputs)\n",
    "\n",
    "        # Final decoder model for inference: takes decoder input and previous states, outputs next token and new states\n",
    "        self.decoder_model = keras.models.Model(\n",
    "            inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "            outputs=[decoder_outputs] + decoder_states\n",
    "        )\n",
    "\n",
    "    def compile(self, optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy']):\n",
    "        # Compile the training model with optimizer, loss, and metrics\n",
    "        self.training_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "    \n",
    "    def fit(self, x, y, batch_size=64, epochs=10, validation_split=0):\n",
    "        # Train the model on the provided data\n",
    "        self.training_model.fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "        )\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input_seqs,\n",
    "        target_seqs,\n",
    "        start_token,\n",
    "        end_token,\n",
    "        max_dec_len,\n",
    "        batch_size=64):\n",
    "        \"\"\"\n",
    "        Batched beam search decoding + exact‐match accuracy.\n",
    "        Uses one big GPU call per time‐step over all (batch×beam) hypotheses.\n",
    "        \"\"\"\n",
    "        N = input_seqs.shape[0]\n",
    "        n_batches = math.ceil(N / batch_size)\n",
    "        total_correct = 0\n",
    "        predictions = []\n",
    "\n",
    "        for bi in range(n_batches):\n",
    "            # Prepare batch for this iteration\n",
    "            batch_inputs = input_seqs[bi*batch_size : (bi+1)*batch_size]\n",
    "            bsz = batch_inputs.shape[0]\n",
    "\n",
    "            # Get encoder states for the batch\n",
    "            enc_states = self.encoder_model.predict(batch_inputs, verbose=0)\n",
    "\n",
    "            B = self.beam_width\n",
    "            flat_states = []\n",
    "            # Tile encoder states for beam search (repeat for each beam)\n",
    "            for state in enc_states:\n",
    "                tiled = np.repeat(state[:, None, :], B, axis=1)\n",
    "                flat_states.append(tiled.reshape(bsz*B, -1))\n",
    "\n",
    "            # Initialize decoder input with start token for each beam\n",
    "            flat_dec_input = np.full((bsz*B, 1), start_token, dtype='int32')\n",
    "\n",
    "            # Initialize sequences and scores for each beam\n",
    "            seqs   = [[[start_token]] * B for _ in range(bsz)]\n",
    "            scores = np.zeros((bsz, B), dtype=np.float32)\n",
    "\n",
    "            for t in range(max_dec_len):\n",
    "                # Prepare inputs for decoder: current token and all states\n",
    "                inputs = [flat_dec_input] + flat_states\n",
    "                outs         = self.decoder_model.predict(inputs, verbose=0)\n",
    "                logits       = outs[0]\n",
    "                next_lp      = np.log(logits[:,0,:] + 1e-9)  # log-probabilities for next token\n",
    "                next_lp      = next_lp.reshape(bsz, B, -1)\n",
    "\n",
    "                init_states  = flat_states\n",
    "                state_outputs= outs[1:len(init_states)+1]\n",
    "                new_seqs     = []\n",
    "                new_scores   = []\n",
    "                new_states   = [np.zeros_like(s) for s in init_states]\n",
    "\n",
    "                for i in range(bsz):\n",
    "                    # For each item in batch, compute new beam candidates\n",
    "                    total_lp    = scores[i][:,None] + next_lp[i]\n",
    "                    flat_idx    = total_lp.reshape(-1)\n",
    "                    topk_idx    = np.argpartition(-flat_idx, B-1)[:B]\n",
    "                    topk_scores = flat_idx[topk_idx]\n",
    "                    prev_beam   = topk_idx // next_lp.shape[2]\n",
    "                    token_id    = topk_idx %  next_lp.shape[2]\n",
    "\n",
    "                    bs_seqs = []\n",
    "                    for j, (bprev, tok) in enumerate(zip(prev_beam, token_id)):\n",
    "                        # Extend previous sequence with new token\n",
    "                        bs_seqs.append(seqs[i][bprev] + [int(tok)])\n",
    "                        src = i*B + bprev\n",
    "                        dst = i*B + j\n",
    "                        # Update decoder states for new beam\n",
    "                        for k, st in enumerate(state_outputs):\n",
    "                            new_states[k][dst] = st[src]\n",
    "\n",
    "                    new_seqs.append(bs_seqs)\n",
    "                    new_scores.append(topk_scores)\n",
    "\n",
    "                # Update for next time step\n",
    "                seqs        = new_seqs\n",
    "                scores      = np.stack(new_scores, axis=0)\n",
    "                flat_states = [s.reshape(bsz*B, -1) for s in new_states]\n",
    "                # Prepare next decoder input (last token of each beam)\n",
    "                flat_dec_input = np.array([[s[-1] for s in bs] for bs in seqs]).reshape(-1,1)\n",
    "\n",
    "                # Early stopping if all beams in all batches ended\n",
    "                if all(s[-1] == end_token for bs in seqs for s in bs):\n",
    "                    break\n",
    "\n",
    "            batch_preds = []\n",
    "            # For each batch item, pick the best beam, remove start token, trim/pad to max_dec_len\n",
    "            for bs in seqs:\n",
    "                best_idx = int(np.argmax([scores[i,j] for j in range(B)]))\n",
    "                seq = bs[best_idx]\n",
    "                if seq and seq[0] == start_token:\n",
    "                    seq = seq[1:]\n",
    "                if end_token in seq:\n",
    "                    seq = seq[:seq.index(end_token)+1]\n",
    "                seq += [0] * (max_dec_len - len(seq))\n",
    "                batch_preds.append(seq)\n",
    "\n",
    "            # Compare predictions to targets for accuracy\n",
    "            tgt_slice = target_seqs[bi*batch_size : bi*batch_size+bsz]\n",
    "            for p, t in zip(batch_preds, tgt_slice):\n",
    "                if np.array_equal(p, t):\n",
    "                    total_correct += 1\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "        return total_correct / N, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-05-17T09:59:53.873611Z",
     "iopub.status.busy": "2025-05-17T09:59:53.873323Z",
     "iopub.status.idle": "2025-05-17T09:59:53.896649Z",
     "shell.execute_reply": "2025-05-17T09:59:53.895869Z",
     "shell.execute_reply.started": "2025-05-17T09:59:53.873590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Dense layers for computing attention scores\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V  = Dense(1)\n",
    "\n",
    "    def call(self, query, values, mask=None):\n",
    "        # Expand query and values for broadcasting in attention calculation\n",
    "        q_expanded = tf.expand_dims(query, 2)   # (batch, T_dec, 1, hidden)\n",
    "        v_expanded = tf.expand_dims(values, 1)  # (batch, 1, T_enc, hidden)\n",
    "\n",
    "        # Compute attention scores (energy) using additive attention\n",
    "        score = self.V(tf.nn.tanh(self.W1(q_expanded) + self.W2(v_expanded)))\n",
    "\n",
    "        # Optionally mask out padding positions in encoder\n",
    "        if mask is not None and mask[1] is not None:\n",
    "            enc_mask = tf.expand_dims(mask[1], 1)\n",
    "            score -= (1.0 - tf.cast(enc_mask, score.dtype)) * 1e9\n",
    "\n",
    "        # Softmax over encoder time axis to get attention weights\n",
    "        attn_weights = tf.nn.softmax(score, axis=2)\n",
    "        attn_weights = tf.squeeze(attn_weights, -1)  # (batch, T_dec, T_enc)\n",
    "        # Weighted sum of encoder outputs (context vector)\n",
    "        context = tf.matmul(attn_weights, values)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "class Seq2SeqAttention:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        output_vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.0,\n",
    "        recurrent_dropout_rate=0.0,\n",
    "        encoder_type='LSTM',\n",
    "        decoder_type='LSTM',\n",
    "        beam_width = 1\n",
    "    ):\n",
    "        # Store model hyperparameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.recurrent_dropout_rate = recurrent_dropout_rate\n",
    "        self.encoder_type = encoder_type\n",
    "        self.decoder_type = decoder_type\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "    def build_training_model(self):\n",
    "        # Encoder input and embedding\n",
    "        enc_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "        enc_emb = Embedding(\n",
    "            self.input_vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            name='encoder_embedding'\n",
    "        )(enc_inputs)\n",
    "\n",
    "        # Encoder RNN (LSTM/GRU/RNN)\n",
    "        EncoderCell = getattr(tf.keras.layers, self.encoder_type)\n",
    "        self.encoder_rnn = EncoderCell(\n",
    "            self.hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            dropout=self.dropout_rate,\n",
    "            recurrent_dropout=self.recurrent_dropout_rate,\n",
    "            name='encoder_'+self.encoder_type.lower()\n",
    "        )\n",
    "        enc_outputs_and_states = self.encoder_rnn(enc_emb)\n",
    "        enc_outputs, *enc_states = enc_outputs_and_states\n",
    "\n",
    "        # Decoder input and embedding\n",
    "        dec_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "        dec_emb = Embedding(\n",
    "            self.output_vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            name='decoder_embedding'\n",
    "        )(dec_inputs)\n",
    "\n",
    "        # Decoder RNN (LSTM/GRU/RNN), initialized with encoder states\n",
    "        DecoderCell = getattr(tf.keras.layers, self.decoder_type)\n",
    "        self.decoder_rnn = DecoderCell(\n",
    "            self.hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            dropout=self.dropout_rate,\n",
    "            recurrent_dropout=self.recurrent_dropout_rate,\n",
    "            name='decoder_'+self.decoder_type.lower()\n",
    "        )\n",
    "        dec_outputs_and_states = self.decoder_rnn(\n",
    "            dec_emb, initial_state=enc_states\n",
    "        )\n",
    "        dec_outputs, *dec_states = dec_outputs_and_states\n",
    "\n",
    "        # Bahdanau attention layer\n",
    "        self.attention_layer = BahdanauAttention(\n",
    "            self.hidden_units, name='bahdanau_attn'\n",
    "        )\n",
    "        context, _ = self.attention_layer(\n",
    "            dec_outputs, enc_outputs\n",
    "        )\n",
    "\n",
    "        # Concatenate decoder outputs and context vector\n",
    "        concat = Concatenate(axis=-1, name='concat_layer')([dec_outputs, context])\n",
    "        # Output layer: project to vocabulary size\n",
    "        dec_logits = Dense(\n",
    "            self.output_vocab_size,\n",
    "            activation='softmax',\n",
    "            name='output_dense'\n",
    "        )(concat)\n",
    "\n",
    "        # Define the full training model\n",
    "        self.training_model = Model(\n",
    "            inputs=[enc_inputs, dec_inputs],\n",
    "            outputs=dec_logits,\n",
    "            name='seq2seq_training'\n",
    "        )\n",
    "        \n",
    "    def build_inference_model(self):\n",
    "        # Encoder inference model for prediction\n",
    "        enc_inputs_inf = Input(\n",
    "            shape=(None,), name='encoder_inputs_inf'\n",
    "        )\n",
    "        enc_emb_inf = self.training_model.get_layer('encoder_embedding')(\n",
    "            enc_inputs_inf\n",
    "        )\n",
    "        enc_rnn = self.training_model.get_layer(\n",
    "            'encoder_'+self.encoder_type.lower()\n",
    "        )\n",
    "        enc_outputs_and_states = enc_rnn(enc_emb_inf)\n",
    "        enc_outputs_inf, *enc_states_inf = enc_outputs_and_states\n",
    "\n",
    "        self.encoder_model = Model(\n",
    "            inputs=enc_inputs_inf,\n",
    "            outputs=[enc_outputs_inf] + enc_states_inf,\n",
    "            name='encoder_inference'\n",
    "        )\n",
    "\n",
    "        # Decoder inference model for step-by-step prediction\n",
    "        dec_token_inf   = Input(shape=(1,), name='decoder_token_inf')\n",
    "        enc_outputs_inp = Input(\n",
    "            shape=(None, self.hidden_units),\n",
    "            name='encoder_outputs_inf'\n",
    "        )\n",
    "\n",
    "        # Decoder state inputs for each state (h, c)\n",
    "        dec_state_inputs = [\n",
    "            Input(shape=(self.hidden_units,), name=f'decoder_state_inf_{i}')\n",
    "            for i in range(len(enc_states_inf))\n",
    "        ]\n",
    "\n",
    "        dec_emb_inf = self.training_model.get_layer('decoder_embedding')(\n",
    "            dec_token_inf\n",
    "        )\n",
    "        dec_rnn = self.training_model.get_layer(\n",
    "            'decoder_'+self.decoder_type.lower()\n",
    "        )\n",
    "        dec_outputs_and_states_inf = dec_rnn(\n",
    "            dec_emb_inf, initial_state=dec_state_inputs\n",
    "        )\n",
    "        dec_out_step, *dec_states_out = dec_outputs_and_states_inf\n",
    "\n",
    "        # Compute context and output for this step\n",
    "        context_inf, _ = self.attention_layer(\n",
    "            dec_out_step, enc_outputs_inp\n",
    "        )\n",
    "        concat_inf = Concatenate(axis=-1)([dec_out_step, context_inf])\n",
    "        dec_logits_inf = self.training_model.get_layer('output_dense')(\n",
    "            concat_inf\n",
    "        )\n",
    "\n",
    "        # Final decoder inference model\n",
    "        self.decoder_model = Model(\n",
    "            inputs=[dec_token_inf, enc_outputs_inp] + dec_state_inputs,\n",
    "            outputs=[dec_logits_inf] + dec_states_out,\n",
    "            name='decoder_inference'\n",
    "        )\n",
    "\n",
    "    def build_attention_extractor(self):\n",
    "        # Build a decoder model that also outputs attention weights for visualization\n",
    "        dec_token_inf   = Input(shape=(1,), name='decoder_token_inf_attn')\n",
    "        enc_outputs_inp = Input(\n",
    "            shape=(None, self.hidden_units),\n",
    "            name='encoder_outputs_inf_attn'\n",
    "        )\n",
    "        dec_state_inputs = [\n",
    "            Input(shape=(self.hidden_units,), name=f'decoder_state_inf_attn_{i}')\n",
    "            for i in range(len(self.encoder_model.output) - 1)\n",
    "        ]\n",
    "        dec_emb_inf = self.training_model.get_layer('decoder_embedding')(dec_token_inf)\n",
    "        dec_rnn = self.training_model.get_layer('decoder_'+self.decoder_type.lower())\n",
    "        dec_outputs_and_states_inf = dec_rnn(dec_emb_inf, initial_state=dec_state_inputs)\n",
    "        dec_out_step, *dec_states_out = dec_outputs_and_states_inf\n",
    "\n",
    "        # Get attention weights for this step\n",
    "        context_inf, attn_weights = self.attention_layer(dec_out_step, enc_outputs_inp)\n",
    "        concat_inf = Concatenate(axis=-1)([dec_out_step, context_inf])\n",
    "        dec_logits_inf = self.training_model.get_layer('output_dense')(concat_inf)\n",
    "\n",
    "        # Decoder model for extracting attention weights\n",
    "        self.decoder_attn_model = Model(\n",
    "            inputs=[dec_token_inf, enc_outputs_inp] + dec_state_inputs,\n",
    "            outputs=[dec_logits_inf, attn_weights] + dec_states_out,\n",
    "            name='decoder_inference_attn'\n",
    "        )\n",
    "\n",
    "    def compile(self, optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy']):\n",
    "        # Compile the training model with optimizer, loss, and metrics\n",
    "        self.training_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "    \n",
    "    def fit(self, x, y, batch_size=64, epochs=10, validation_split=0):\n",
    "        # Train the model on the provided data\n",
    "        self.training_model.fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "        )\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        input_seqs,\n",
    "        target_seqs,\n",
    "        start_token,\n",
    "        end_token,\n",
    "        max_dec_len,\n",
    "        batch_size=64\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Batched beam search decoding + exact match accuracy.\n",
    "        Uses one big GPU call per timestep over all (batch beam) hypotheses.\n",
    "        \"\"\"\n",
    "        N = input_seqs.shape[0]\n",
    "        n_batches = math.ceil(N / batch_size)\n",
    "        total_correct = 0\n",
    "        predictions = []\n",
    "\n",
    "        for bi in range(n_batches):\n",
    "            batch_inputs = input_seqs[bi*batch_size : (bi+1)*batch_size]\n",
    "            bsz = batch_inputs.shape[0]\n",
    "\n",
    "            # Encoder returns [encoder_outputs, state_h, state_c] (for LSTM)\n",
    "            enc_results = self.encoder_model.predict(batch_inputs, verbose=0)\n",
    "            enc_outputs = enc_results[0]\n",
    "            enc_states = enc_results[1:]\n",
    "\n",
    "            B = self.beam_width\n",
    "            # Tile encoder outputs and states for beam search\n",
    "            flat_enc_outputs = np.repeat(enc_outputs[:, None, :, :], B, axis=1).reshape(bsz*B, enc_outputs.shape[1], enc_outputs.shape[2])\n",
    "            flat_states = []\n",
    "            for state in enc_states:\n",
    "                tiled = np.repeat(state[:, None, :], B, axis=1)\n",
    "                flat_states.append(tiled.reshape(bsz*B, -1))\n",
    "\n",
    "            flat_dec_input = np.full((bsz*B, 1), start_token, dtype='int32')\n",
    "\n",
    "            seqs   = [[[start_token]] * B for _ in range(bsz)]\n",
    "            scores = np.zeros((bsz, B), dtype=np.float32)\n",
    "\n",
    "            for t in range(max_dec_len):\n",
    "                # Decoder expects: [dec_token, enc_outputs, *states]\n",
    "                inputs = [flat_dec_input, flat_enc_outputs] + flat_states\n",
    "                outs   = self.decoder_model.predict(inputs, verbose=0)\n",
    "                logits = outs[0]\n",
    "                next_lp = np.log(logits[:,0,:] + 1e-9)\n",
    "\n",
    "                next_lp = next_lp.reshape(bsz, B, -1)\n",
    "\n",
    "                new_seqs  = []\n",
    "                new_scores = []\n",
    "                new_states = [np.zeros_like(s) for s in flat_states]\n",
    "\n",
    "                for i in range(bsz):\n",
    "                    total_lp = scores[i][:, None] + next_lp[i]\n",
    "                    flat_indices = total_lp.reshape(-1)\n",
    "\n",
    "                    topk_idx = np.argpartition(-flat_indices, B-1)[:B]\n",
    "                    topk_scores = flat_indices[topk_idx]\n",
    "\n",
    "                    prev_beam = topk_idx // next_lp.shape[2]\n",
    "                    token_id  = topk_idx %  next_lp.shape[2]\n",
    "\n",
    "                    bs_seqs = []\n",
    "                    for j, (bprev, tok) in enumerate(zip(prev_beam, token_id)):\n",
    "                        # Extend previous sequence with new token\n",
    "                        seq = seqs[i][bprev] + [int(tok)]\n",
    "                        bs_seqs.append(seq)\n",
    "\n",
    "                        src_idx = i*B + bprev\n",
    "                        dst_idx = i*B + j\n",
    "                        # Update decoder states for new beam\n",
    "                        for k, st in enumerate(outs[1:]):\n",
    "                            new_states[k][dst_idx] = st[src_idx]\n",
    "\n",
    "                    new_seqs.append(bs_seqs)\n",
    "                    new_scores.append(topk_scores)\n",
    "\n",
    "                seqs   = new_seqs\n",
    "                scores = np.stack(new_scores, axis=0)\n",
    "                flat_states = [ns.reshape(bsz*B, -1) for ns in new_states]\n",
    "                last_tokens = [ [s[-1] for s in bs] for bs in seqs ]\n",
    "                flat_dec_input = np.array(last_tokens).reshape(-1,1)\n",
    "\n",
    "                # Early stopping if all beams in all batches ended\n",
    "                if all(s[-1] == end_token for bs in seqs for s in bs):\n",
    "                    break\n",
    "\n",
    "            batch_preds = []\n",
    "            for i, bs in enumerate(seqs):\n",
    "                best_idx = int(np.argmax(scores[i]))\n",
    "                seq = bs[best_idx]\n",
    "                if seq and seq[0] == start_token:\n",
    "                    seq = seq[1:]\n",
    "                # Remove start token, trim at end token, pad to max_dec_len\n",
    "                if end_token in seq:\n",
    "                    seq = seq[:seq.index(end_token)+1]\n",
    "                seq += [0] * (max_dec_len - len(seq))\n",
    "                batch_preds.append(seq)\n",
    "\n",
    "            tgt_slice = target_seqs[bi*batch_size : bi*batch_size+bsz]\n",
    "            for p, t in zip(batch_preds, tgt_slice):\n",
    "                if np.array_equal(p, t):\n",
    "                    total_correct += 1\n",
    "\n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "        return total_correct / N, predictions\n",
    "\n",
    "    def get_attention_for_example(self, input_seq, start_token, end_token, max_dec_len):\n",
    "        # Encode input sequence to get encoder outputs and states\n",
    "        enc_results = self.encoder_model.predict(input_seq[None, :], verbose=0)\n",
    "        enc_outputs = enc_results[0]\n",
    "        enc_states = enc_results[1:]\n",
    "\n",
    "        dec_input = np.array([[start_token]])\n",
    "        states = [s for s in enc_states]\n",
    "        attn_matrices = []\n",
    "\n",
    "        output_seq = []\n",
    "        for _ in range(max_dec_len):\n",
    "            # Run one decoding step and get attention weights\n",
    "            outs = self.decoder_attn_model.predict([dec_input, enc_outputs] + states, verbose=0)\n",
    "            logits, attn_weights, *states = outs\n",
    "            pred_token = int(np.argmax(logits[0, 0]))\n",
    "            output_seq.append(pred_token)\n",
    "            attn_matrices.append(attn_weights[0])  # shape: (1, input_len)\n",
    "            if pred_token == end_token:\n",
    "                break\n",
    "            dec_input = np.array([[pred_token]])\n",
    "\n",
    "        # Trim output_seq and attn_matrices at first 0 (padding)\n",
    "        if 0 in output_seq:\n",
    "            idx = output_seq.index(0)\n",
    "            output_seq = output_seq[:idx]\n",
    "            attn_matrices = attn_matrices[:idx]\n",
    "            \n",
    "        attn_matrices = np.stack([np.squeeze(a) for a in attn_matrices], axis=0)\n",
    "\n",
    "        # Trim attention columns for padded input tokens\n",
    "        if isinstance(input_seq, np.ndarray):\n",
    "            input_seq = input_seq.tolist()\n",
    "        input_len = input_seq.index(0) if 0 in input_seq else len(input_seq)\n",
    "        attn_matrices = attn_matrices[:, :input_len]\n",
    "\n",
    "        return output_seq, attn_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0; change if needed\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs found: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login 07768b2858c526bbe80eda67a46304a19f697af1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "        return f\"<span style='color:#000;padding-left:10px;background-color:{color}'>&nbsp;</span>\"\n",
    "    else:\n",
    "        return f\"<span style='color:#000;background-color:{color}'>{s} </span>\"\n",
    "\n",
    "def print_color(t):\n",
    "    return ''.join([cstr(ti, color=ci) for ti,ci in t])\n",
    "\n",
    "def get_clr(value):\n",
    "    colors = [ '#FFFFFF', '#FFFFFF', '#FFFFFF', '#FFFFFF', '#f9e8e8', '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f', '#f68f8f', '#f47676', '#f45f5f', '#f45f5f', '#f34343', '#f34343', '#f33b3b', '#f33b3b', '#f33b3b', '#f42e2e', '#f42e2e']\n",
    "    value = int((value * 100) / 5)\n",
    "    value = min(value, len(colors)-1)\n",
    "    return colors[value]\n",
    "\n",
    "def load_attention_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# Load all three attention files\n",
    "attn_files = {\n",
    "    \"LSTM\": \"attention_data_LSTM.jsonl\",\n",
    "    \"GRU\": \"attention_data_GRU.jsonl\",\n",
    "    \"SimpleRNN\": \"attention_data_SimpleRNN.jsonl\"\n",
    "}\n",
    "attn_data = {k: load_attention_file(v) for k, v in attn_files.items()}\n",
    "\n",
    "\n",
    "def get_by_idx(data, idx):\n",
    "    for item in data:\n",
    "        if item[\"idx\"] == idx:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "def cstr_hover(s, color='black', hover_text=None):\n",
    "    # Use title attribute for hover\n",
    "    hover = f\" title='{hover_text}'\" if hover_text else \"\"\n",
    "    return f\"<span style='color:#000;background-color:{color};padding:2px 6px;border-radius:3px;margin:1px;cursor:pointer;' {hover}>{s}</span>\"\n",
    "\n",
    "def print_color_hover(t):\n",
    "    # t: list of (char, color, hover_text)\n",
    "    return ''.join([cstr_hover(ti, color=ci, hover_text=ht) for ti,ci,ht in t])\n",
    "\n",
    "def generate_html_for_step(items, step):\n",
    "    html = \"<div style='display:flex;gap:40px;margin-bottom:40px;'>\"\n",
    "    for name in attn_files.keys():\n",
    "        if name not in items:\n",
    "            html += f\"<div style='min-width:250px;'><b>{name}</b>: Not found</div>\"\n",
    "            continue\n",
    "        attn_weights = items[name][\"attn_weights\"]\n",
    "        outputs = items[name][\"outputs\"]\n",
    "        input_labels = items[name][\"input_labels\"]\n",
    "        if step >= len(attn_weights):\n",
    "            html += f\"<div style='min-width:250px;'><b>{name}</b>: Done</div>\"\n",
    "            continue\n",
    "        # Input row (no hover)\n",
    "        text_ce = []\n",
    "        for j in range(len(input_labels)):\n",
    "            text_e = (input_labels[j], get_clr(attn_weights[step][j]), None)\n",
    "            text_ce.append(text_e)\n",
    "        # Output row (hover shows distribution)\n",
    "        text_he = []\n",
    "        for j in range(len(outputs)):\n",
    "            # Only highlight the output at this step, others are gray/white\n",
    "            if j == step and j < len(attn_weights) and j < len(attn_weights[step]):\n",
    "                color = get_clr(attn_weights[step][j])\n",
    "            else:\n",
    "                color = get_clr(0)\n",
    "            # Prepare hover text: show attention distribution for this output step\n",
    "            if j < len(attn_weights):\n",
    "                dist = \", \".join(f\"{input_labels[k]}: {attn_weights[j][k]:.2f}\" for k in range(len(input_labels)))\n",
    "            else:\n",
    "                dist = \"\"\n",
    "            text_h = (outputs[j], color, dist)\n",
    "            text_he.append(text_h)\n",
    "        block = f\"<b>{name}</b><br>\"\n",
    "        block += print_color_hover(text_he) + \"<br>\" + print_color_hover(text_ce)\n",
    "        html += f\"<div style='min-width:250px;'>{block}</div>\"\n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "def export_parallel_visualization_html(idx, out_path=\"attention_comparison.html\"):\n",
    "    items = {}\n",
    "    max_steps = 0\n",
    "    for name, data in attn_data.items():\n",
    "        item = get_by_idx(data, idx)\n",
    "        if item is None:\n",
    "            continue\n",
    "        attn_weights = np.array(item[\"attention\"])\n",
    "        outputs = item[\"output_labels\"]\n",
    "        input_labels = item[\"input_labels\"]\n",
    "        if outputs:\n",
    "            outputs = outputs[:-1] + ['<e>']\n",
    "        attn_weights = attn_weights[:-1]\n",
    "        for i in range(len(attn_weights)):\n",
    "            attn_weights[i] = attn_weights[i][:len(input_labels)]\n",
    "        attn_weights = np.asarray(attn_weights)\n",
    "        items[name] = {\n",
    "            \"attn_weights\": attn_weights,\n",
    "            \"outputs\": outputs,\n",
    "            \"input_labels\": input_labels\n",
    "        }\n",
    "        max_steps = max(max_steps, len(attn_weights))\n",
    "\n",
    "    html = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset='utf-8'>\n",
    "    <title>Hover Over the Character to see the distribution of attention from Input</title>\n",
    "    <style>\n",
    "    body { font-family: 'Nirmala UI', 'Gujarati Saral', 'Mangal', sans-serif; }\n",
    "    .attn-row { display: flex; gap: 40px; margin-bottom: 40px; }\n",
    "    .attn-box { min-width: 250px; }\n",
    "    .attn-title { font-weight: bold; font-size: 1.1em; margin-bottom: 6px; }\n",
    "    .attn-output { font-size: 1.2em; margin-bottom: 6px; }\n",
    "    .attn-input { font-size: 1.1em; margin-bottom: 6px; }\n",
    "    .attn-bar { margin-top: 10px; }\n",
    "    .attn-cell { display: inline-block; padding: 2px 6px; border-radius: 3px; margin: 1px; min-width: 24px; text-align: center; }\n",
    "    .step-section { display: none; }\n",
    "    </style>\n",
    "    <script>\n",
    "    function showAttn(id) {\n",
    "        var bars = document.getElementsByClassName('attn-bar');\n",
    "        for (var i = 0; i < bars.length; ++i) bars[i].style.display = 'none';\n",
    "        var el = document.getElementById(id);\n",
    "        if (el) el.style.display = 'block';\n",
    "    }\n",
    "    function hideAttn(id) {\n",
    "        var el = document.getElementById(id);\n",
    "        if (el) el.style.display = 'none';\n",
    "    }\n",
    "    var currentStep = 0;\n",
    "    function showStep(idx, total) {\n",
    "        for (var i = 0; i < total; ++i) {\n",
    "            var sec = document.getElementById('step_section_' + i);\n",
    "            if (sec) sec.style.display = (i == idx ? 'block' : 'none');\n",
    "        }\n",
    "        document.getElementById('step_num').innerText = (idx+1) + ' / ' + total;\n",
    "        currentStep = idx;\n",
    "    }\n",
    "    function nextStep(total) {\n",
    "        if (currentStep < total-1) showStep(currentStep+1, total);\n",
    "    }\n",
    "    function prevStep(total) {\n",
    "        if (currentStep > 0) showStep(currentStep-1, total);\n",
    "    }\n",
    "    </script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h2>Attention Visualization Comparison</h2>\n",
    "    <div>\n",
    "      <button onclick=\"prevStep({max_steps})\">&lt; Prev</button>\n",
    "      <span id=\"step_num\">1 / {max_steps}</span>\n",
    "      <button onclick=\"nextStep({max_steps})\">Next &gt;</button>\n",
    "    </div>\n",
    "    \"\"\".replace(\"{max_steps}\", str(max_steps))\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        html += f\"<div class='step-section' id='step_section_{step}' style='display:{'block' if step==0 else 'none'};'>\"\n",
    "        html += f\"<h3>Step {step+1}</h3>\"\n",
    "        html += \"<div class='attn-row'>\"\n",
    "        for name in attn_files.keys():\n",
    "            if name not in items:\n",
    "                html += f\"<div class='attn-box'><span class='attn-title'>{name}</span><br>Not found</div>\"\n",
    "                continue\n",
    "            attn_weights = items[name][\"attn_weights\"]\n",
    "            outputs = items[name][\"outputs\"]\n",
    "            input_labels = items[name][\"input_labels\"]\n",
    "            if step >= len(attn_weights):\n",
    "                html += f\"<div class='attn-box'><span class='attn-title'>{name}</span><br>Done</div>\"\n",
    "                continue\n",
    "\n",
    "            # Output row with hover\n",
    "            output_html = \"\"\n",
    "            attn_bars_html = \"\"\n",
    "            for j, out_char in enumerate(outputs):\n",
    "                attn_id = f\"attn_{name}_{step}_{j}\"\n",
    "                if j < len(attn_weights):\n",
    "                    bar = \"<div class='attn-bar' id='{0}' style='display:none;'>\".format(attn_id)\n",
    "                    for k, in_char in enumerate(input_labels):\n",
    "                        color = get_clr(attn_weights[j][k])\n",
    "                        val = attn_weights[j][k]\n",
    "                        bar += f\"<span class='attn-cell' style='background:{color}' title='{in_char}: {val:.2f}'>{in_char}<br><span style='font-size:0.8em'>{val:.2f}</span></span>\"\n",
    "                    bar += \"</div>\"\n",
    "                    attn_bars_html += bar\n",
    "                    if j == step and j < attn_weights.shape[1]:\n",
    "                        bg_color = get_clr(attn_weights[step][j])\n",
    "                    else:\n",
    "                        bg_color = get_clr(0)\n",
    "                    output_html += f\"<span class='attn-cell' style='background:{bg_color};cursor:pointer;' onmouseover=\\\"showAttn('{attn_id}')\\\" onmouseout=\\\"hideAttn('{attn_id}')\\\">{out_char}</span>\"\n",
    "                else:\n",
    "                    output_html += f\"<span class='attn-cell'>{out_char}</span>\"\n",
    "            input_html = \"\"\n",
    "            for k, in_char in enumerate(input_labels):\n",
    "                input_html += f\"<span class='attn-cell'>{in_char}</span>\"\n",
    "\n",
    "            html += f\"<div class='attn-box'><div class='attn-title'>{name}</div>\"\n",
    "            html += f\"<div class='attn-output'>{output_html}</div>\"\n",
    "            html += f\"<div class='attn-input'>{input_html}</div>\"\n",
    "            html += attn_bars_html\n",
    "            html += \"</div>\"\n",
    "        html += \"</div></div>\"\n",
    "    html += \"\"\"\n",
    "    <script>showStep(0, {max_steps});</script>\n",
    "    </body></html>\n",
    "    \"\"\".replace(\"{max_steps}\", str(max_steps))\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decoder_activations_multi_combined(json_path, neuron_idx=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    import json\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Collect all neuron indices\n",
    "    all_neurons = set()\n",
    "    for example in data:\n",
    "        all_neurons.update(example[\"neuron_indices\"])\n",
    "    if neuron_idx is not None:\n",
    "        all_neurons = set([neuron_idx])\n",
    "\n",
    "    for nidx in sorted(all_neurons):\n",
    "        # Filter examples that have this neuron\n",
    "        filtered = [ex for ex in data if str(nidx) in ex[\"activations\"]]\n",
    "        if not filtered:\n",
    "            continue\n",
    "        n_examples = len(filtered)\n",
    "        # Use fixed cell size: width = cell_width * max_T_dec, height = cell_height * n_examples\n",
    "        cell_width = 0.7\n",
    "        cell_height = 1.2\n",
    "        max_T_dec = max(len(ex[\"decoded_chars\"]) for ex in filtered)\n",
    "        fig, axes = plt.subplots(\n",
    "            n_examples, 1, figsize=(cell_width * max_T_dec, cell_height * n_examples),\n",
    "            squeeze=False\n",
    "        )\n",
    "        fig.suptitle(f\"Neuron {nidx} activations across {n_examples} examples\", fontsize=16)\n",
    "        for i, example in enumerate(filtered):\n",
    "            decoded_chars = example[\"decoded_chars\"]\n",
    "            acts = example[\"activations\"][str(nidx)]\n",
    "            abs_vals = np.abs(acts)\n",
    "            vmin = 0\n",
    "            vmax = max(abs_vals)\n",
    "            norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "            cmap = plt.cm.RdBu\n",
    "            ax = axes[i, 0]\n",
    "            ax.axis('off')\n",
    "            for j, (ch, act) in enumerate(zip(decoded_chars, acts)):\n",
    "                facecolor = cmap(norm(abs(act)))\n",
    "                rect = mpl.patches.Rectangle((j, 0), 1, 1, facecolor=facecolor, edgecolor='gray', linewidth=0.5)\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(\n",
    "                    j + 0.5, 0.5, ch,\n",
    "                    fontsize=16,\n",
    "                    ha='center', va='center',\n",
    "                    color='black'  # Always black\n",
    "                )\n",
    "            ax.set_xlim(0, max_T_dec)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_title(f\"Example idx={example['idx']}\", fontsize=11)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to get the config value from wandb or command line arguments\n",
    "def get_config_value(config, args, key, default=None):\n",
    "    return getattr(config, key, getattr(args, key, default))\n",
    "\n",
    "def train_model(config=None):\n",
    "    # set default hyperparameters\n",
    "    defaults = {\n",
    "        'embedding_dim': 32,\n",
    "        'hidden_units': 256,\n",
    "        'dropout_rate': 0.0640614879476808,\n",
    "        'recurrent_dropout_rate': 0.14003195916675987,\n",
    "        'encoder_layers': 3,\n",
    "        'decoder_layers': 3,\n",
    "        'cell_type': 'LSTM',\n",
    "        'beam_width': 3,\n",
    "        'dataset': '/kaggle/input/as3-dataset/lexicons',\n",
    "        'learning_rate': 0.0041760877805365965,\n",
    "        'attention': False,\n",
    "        'do_val':False,\n",
    "        'do_test': False,\n",
    "        'epochs': 1,\n",
    "        'attention_extractor': False,\n",
    "        'connectivity': False,\n",
    "        'LSTM_cell_vis': False,\n",
    "    }\n",
    "\n",
    "    # Initialize wandb with the provided entity and project\n",
    "    with wandb.init(entity='me21b138-indian-institute-of-technology-madras', project='AS3', config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Create a class to mimic argparse for the helper functions\n",
    "        class Args:\n",
    "            def __init__(self, **kwargs):\n",
    "                for key, value in kwargs.items():\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "        # Set up args with defaults\n",
    "        args = Args(**defaults)\n",
    "        \n",
    "        data = extract_data()\n",
    "        train_data = data['train']\n",
    "        dev_data = data['dev']\n",
    "        test_data = data['test']\n",
    "\n",
    "        # Set up the tokenizer for the encoder and decoder\n",
    "        encoder_tokenizer = tokanize_texts(np.concatenate((dev_data[:,1], train_data[:,1]), axis=0))\n",
    "        decoder_tokenizer = tokanize_texts(np.concatenate((dev_data[:,0], train_data[:,0]), axis=0), start_end_tokens=True)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        train_y = decoder_tokenizer.texts_to_sequences(add_start_end(train_data[:,0]))\n",
    "        dev_y = decoder_tokenizer.texts_to_sequences(add_start_end(dev_data[:,0]))\n",
    "        test_y = decoder_tokenizer.texts_to_sequences(add_start_end(test_data[:,0]))\n",
    "\n",
    "        train_x = encoder_tokenizer.texts_to_sequences(train_data[:,1])\n",
    "        dev_x = encoder_tokenizer.texts_to_sequences(dev_data[:,1])\n",
    "        test_x = encoder_tokenizer.texts_to_sequences(test_data[:,1])\n",
    "\n",
    "        max_encoder_seq_length = max([len(seq) for seq in train_x + dev_x])\n",
    "        max_decoder_seq_length = max([len(seq) for seq in train_y + dev_y])\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        train_x = pad_sequences(train_x, maxlen=max_encoder_seq_length, padding='post')\n",
    "        dev_x = pad_sequences(dev_x, maxlen=max_encoder_seq_length, padding='post')\n",
    "        test_x = pad_sequences(test_x, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "        train_y = pad_sequences(train_y, maxlen=max_decoder_seq_length, padding='post')\n",
    "        dev_y = pad_sequences(dev_y, maxlen=max_decoder_seq_length, padding='post')\n",
    "        test_y = pad_sequences(test_y, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "        input_vocab_size = len(encoder_tokenizer.word_index) + 1\n",
    "        output_vocab_size = len(decoder_tokenizer.word_index) + 1\n",
    "        train_y_cat = np.eye(output_vocab_size)[train_y]\n",
    "\n",
    "        run_name = f\"{get_config_value(config, args, 'cell_type')}_{get_config_value(config, args, 'attention')}_{get_config_value(config, args, 'beam_width')}_{get_config_value(config, args, 'embedding_dim')}_{get_config_value(config, args, 'hidden_units')}_learning_rate_{get_config_value(config, args, 'learning_rate')}\"\n",
    "        wandb.run.name=run_name\n",
    "        model = None\n",
    "        \n",
    "        # Check if attention is enabled\n",
    "        if get_config_value(config, args, 'attention'):\n",
    "            model = Seq2SeqAttention(\n",
    "                input_vocab_size=input_vocab_size,\n",
    "                output_vocab_size=output_vocab_size,\n",
    "                embedding_dim=get_config_value(config, args, 'embedding_dim'),\n",
    "                hidden_units=get_config_value(config, args, 'hidden_units'),\n",
    "                dropout_rate=get_config_value(config, args, 'dropout_rate'),\n",
    "                recurrent_dropout_rate=get_config_value(config, args, 'recurrent_dropout_rate'),\n",
    "                encoder_type=get_config_value(config, args, 'cell_type'),\n",
    "                decoder_type=get_config_value(config, args, 'cell_type'),\n",
    "                beam_width=get_config_value(config, args, 'beam_width')\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            model = seq2seq(\n",
    "                input_vocab_size=input_vocab_size,\n",
    "                output_vocab_size=output_vocab_size,\n",
    "                embedding_dim=get_config_value(config, args, 'embedding_dim'),\n",
    "                hidden_units=get_config_value(config, args, 'hidden_units'),\n",
    "                encoder_layers=get_config_value(config, args, 'encoder_layers'),\n",
    "                decoder_layers=get_config_value(config, args, 'decoder_layers'),\n",
    "                dropout_rate=get_config_value(config, args, 'dropout_rate'),\n",
    "                recurrent_dropout_rate=get_config_value(config, args, 'recurrent_dropout_rate'),\n",
    "                encoder_type=get_config_value(config, args, 'cell_type'),\n",
    "                decoder_type=get_config_value(config, args, 'cell_type'),\n",
    "                beam_width=get_config_value(config, args, 'beam_width')\n",
    "            )\n",
    "        \n",
    "        model.build_training_model()\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=get_config_value(config, args, 'learning_rate')\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model.fit(\n",
    "            x=[train_x, train_y[:, :-1]],\n",
    "            y=train_y_cat[:, 1:, :],\n",
    "            batch_size=400,\n",
    "            epochs=get_config_value(config, args, 'epochs'),\n",
    "            validation_split=0.0\n",
    "        )\n",
    "        model.build_inference_model()\n",
    "\n",
    "        dev_y_eval = dev_y[:, 1:]\n",
    "        test_y_eval = test_y[:, 1:]\n",
    "\n",
    "        if get_config_value(config, args, 'attention_extractor') or get_config_value(config, args, 'connectivity'):\n",
    "            model.build_attention_extractor()\n",
    "            output_seq, attn_matrices = model.get_attention_for_example(\n",
    "                input_seq=test_x[0],  # shape: (input_len,)\n",
    "                start_token=decoder_tokenizer.word_index.get('§', 1),\n",
    "                end_token=decoder_tokenizer.word_index.get('¶', 0),\n",
    "                max_dec_len=dev_y_eval.shape[1]\n",
    "            )\n",
    "\n",
    "            attention_data = []\n",
    "            indices = [random.randint(0, len(test_x)-1) for _ in range(9)]\n",
    "\n",
    "            reverse_decoder_map = {v: k for k, v in decoder_tokenizer.word_index.items()}\n",
    "            reverse_decoder_map[0] = ''\n",
    "            reverse_encoder_map = {v: k for k, v in encoder_tokenizer.word_index.items()}\n",
    "            reverse_encoder_map[0] = ''\n",
    "\n",
    "            def tokens_to_text_list(tokens, reverse_map, remove_special=None):\n",
    "                if remove_special is None:\n",
    "                    remove_special = []\n",
    "                return [reverse_map.get(tok, '') for tok in tokens if tok > 0 and reverse_map.get(tok, '') not in remove_special]\n",
    "\n",
    "            for idx in indices:\n",
    "                input_seq = test_x[idx]\n",
    "                output_seq, attn_matrices = model.get_attention_for_example(\n",
    "                    input_seq=input_seq,\n",
    "                    start_token=decoder_tokenizer.word_index.get('§', 1),\n",
    "                    end_token=decoder_tokenizer.word_index.get('¶', 0),\n",
    "                    max_dec_len=dev_y.shape[1]\n",
    "                )\n",
    "                input_labels = tokens_to_text_list(input_seq, reverse_encoder_map)\n",
    "                output_labels = tokens_to_text_list(output_seq, reverse_decoder_map, remove_special=['§', '¶', '<e>'])\n",
    "                attn_list = attn_matrices.tolist()  # convert numpy array to list for JSON\n",
    "            \n",
    "                attention_data.append({\n",
    "                    \"idx\": idx,\n",
    "                    \"input_labels\": input_labels,\n",
    "                    \"output_labels\": output_labels,\n",
    "                    \"attention\": attn_list\n",
    "                })            \n",
    "\n",
    "            with open(f\"attention_data_{get_config_value(config, args, 'cell_type')}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in attention_data:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            if get_config_value(config, args, 'connectivity'):\n",
    "                export_parallel_visualization_html(common_idx, out_path=\"attention_comparison.html\")\n",
    "                wandb.log({\"attention_comparison\": wandb.Html(\"attention_comparison.html\")})\n",
    "\n",
    "        if get_config_value(config, args, 'LSTM_cell_vis') and get_config_value(config, args, 'cell_type') == 'LSTM':\n",
    "            decoder_layer_name = \"decoder_0\"   # name of the first decoder LSTM\n",
    "            decoder_lstm_layer = model.training_model.get_layer(decoder_layer_name)\n",
    "            sequence_tensor = decoder_lstm_layer.output[0]  # pick the first output (the full sequence)\n",
    "        \n",
    "            intermediate_decoder_model = Model(\n",
    "                inputs=model.training_model.inputs,   # still [encoder_input, decoder_input]\n",
    "                outputs=sequence_tensor               # just the (batch, T_dec, hidden_units) tensor\n",
    "            )\n",
    "\n",
    "            num_examples = 15\n",
    "            num_neurons = 5\n",
    "            x_enc = np.expand_dims(test_x[0], axis=0)\n",
    "            x_dec_in = np.expand_dims(test_y[0], axis=0)\n",
    "            decoder_seq_outputs = intermediate_decoder_model.predict([x_enc, x_dec_in], verbose=0)  # (1, T_dec, hidden_units)\n",
    "            hidden_units = decoder_seq_outputs.shape[2]\n",
    "            # Pick random indices from test set\n",
    "            random_indices = random.sample(range(len(test_x)), num_examples)\n",
    "            # Pick random neuron indices (make sure < hidden_units)\n",
    "            random_neurons = random.sample(range(hidden_units), num_neurons)\n",
    "\n",
    "            examples_data = []\n",
    "            reverse_decoder_map = {index: char for char, index in decoder_tokenizer.word_index.items()}\n",
    "            reverse_decoder_map[0] = \"\"\n",
    "\n",
    "            for idx in random_indices:\n",
    "                x_enc = np.expand_dims(test_x[idx], axis=0)\n",
    "                x_dec_in = np.expand_dims(test_y[idx], axis=0)\n",
    "                decoder_seq_outputs = intermediate_decoder_model.predict([x_enc, x_dec_in], verbose=0)  # (1, T_dec, hidden_units)\n",
    "                T_dec = decoder_seq_outputs.shape[1]\n",
    "                # Get decoded chars (remove padding and special tokens)\n",
    "                seq = test_y[idx]\n",
    "                decoded_chars = [reverse_decoder_map.get(tok, '') for tok in seq if tok > 0 and reverse_decoder_map.get(tok, '') not in ['§', '¶']]\n",
    "                # Extract activations for selected neurons\n",
    "                activations = {}\n",
    "                for nidx in random_neurons:\n",
    "                    activations[str(nidx)] = decoder_seq_outputs[0, :, nidx].tolist()\n",
    "                examples_data.append({\n",
    "                    \"idx\": idx,\n",
    "                    \"decoded_chars\": decoded_chars,\n",
    "                    \"neuron_indices\": random_neurons,\n",
    "                    \"activations\": activations,\n",
    "                    \"T_dec\": T_dec\n",
    "                })\n",
    "            with open(\"decoder_activations_multi.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(examples_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            plot_decoder_activations_multi_combined(\"decoder_activations_multi.json\")\n",
    "\n",
    "        # Evaluate the model on the validation and test sets\n",
    "        if get_config_value(config, args, 'do_val'):  \n",
    "            dev_acc, dev_prediction = model.evaluate(\n",
    "                input_seqs=dev_x,\n",
    "                target_seqs=dev_y_eval,\n",
    "                start_token=decoder_tokenizer.word_index.get('§', 1),\n",
    "                end_token=decoder_tokenizer.word_index.get('¶', 0),\n",
    "                max_dec_len=dev_y_eval.shape[1],\n",
    "                batch_size=1000\n",
    "            )\n",
    "            wandb.log({\"validation_accuracy\": dev_acc})\n",
    "\n",
    "        if get_config_value(config, args, 'do_test'):\n",
    "            test_acc, test_prediction = model.evaluate(\n",
    "                input_seqs=test_x,\n",
    "                target_seqs=test_y_eval,\n",
    "                start_token=decoder_tokenizer.word_index.get('§', 1),\n",
    "                end_token=decoder_tokenizer.word_index.get('¶', 0),\n",
    "                max_dec_len=dev_y_eval.shape[1],\n",
    "                batch_size=1000\n",
    "            )\n",
    "            wandb.log({\"test_accuracy\": test_acc})\n",
    "\n",
    "            folder_name = \"predictions_attention\" if get_config_value(config, args, 'attention') else \"predictions_vanilla\"\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "            # Reverse mapping from token to char\n",
    "            reverse_decoder_map = {v: k for k, v in decoder_tokenizer.word_index.items()}\n",
    "            reverse_decoder_map[0] = ''  # padding\n",
    "            reverse_encoder_map = {v: k for k, v in encoder_tokenizer.word_index.items()}\n",
    "            reverse_encoder_map[0] = ''  # padding\n",
    "\n",
    "            def tokens_to_text(tokens, reverse_map, remove_special=None):\n",
    "                if remove_special is None:\n",
    "                    remove_special = []\n",
    "                return ''.join([reverse_map.get(tok, '') for tok in tokens if tok > 0 and reverse_map.get(tok, '') not in remove_special])\n",
    "\n",
    "            # Convert test_x, test_y, and predictions to text\n",
    "            x_texts = [tokens_to_text(seq, reverse_encoder_map) for seq in test_x]\n",
    "            y_true_texts = [tokens_to_text(seq, reverse_decoder_map, remove_special=['§', '¶']) for seq in test_y]\n",
    "            y_pred_texts = [tokens_to_text(seq, reverse_decoder_map, remove_special=['§', '¶']) for seq in test_prediction]\n",
    "\n",
    "            df = pd.DataFrame({'x': x_texts, 'true_y': y_true_texts, 'pred_y': y_pred_texts})\n",
    "            csv_path = os.path.join(folder_name, 'predictions.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'validation_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [16, 32, 64, 128, 256, 512]\n",
    "        },\n",
    "        'hidden_units': {\n",
    "            'values': [16, 32, 64, 128, 256, 512]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'recurrent_dropout_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'encoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'decoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['SimpleRNN', 'LSTM', 'GRU']\n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-1\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the sweep\n",
    "entity = 'me21b138-indian-institute-of-technology-madras'  # Your wandb entity\n",
    "project = 'AS3'  # Your wandb project\n",
    "count = 100  # Number of runs to execute\n",
    "\n",
    "# Initialize the sweep\n",
    "wandb.require(\"core\")\n",
    "sweep_id = wandb.sweep(sweep_config, entity=entity, project=project)\n",
    "\n",
    "# Start the sweep agent\n",
    "wandb.agent(sweep_id, function=train_model, count=count)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7429446,
     "sourceId": 11826686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
